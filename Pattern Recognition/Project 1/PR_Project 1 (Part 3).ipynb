{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern Recognition_Project 1(Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Remove html tags\n",
    "\n",
    "https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def cleanhtml(file):\n",
    "  cleanr = re.compile(\"(<.*?>)|(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "  cleantext = re.sub(cleanr, '', file)  \n",
    "  cleantext =''.join([i for i in cleantext if not i.isdigit()])\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all text files from a folder\n",
    "\n",
    "https://automating-gis-processes.github.io/FEC/Day3-reading-files.html\n",
    "\n",
    "Remove stop words\n",
    "\n",
    "https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python\n",
    "\n",
    "https://codereview.stackexchange.com/questions/90692/removing-all-stopwords-from-a-list-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "150\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\hell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\hell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:68: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\hell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:85: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "cachedStopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "DataPathList_Train_Pos = glob.glob('C:/Users/hell/Desktop/term1/Project1 PR/aclImdb_v1/aclImdb/train/pos+/*.txt')\n",
    "print(len(DataPathList_Train_Pos) )  #number of items\n",
    "\n",
    "DataPathList_Train_Neg = glob.glob('C:/Users/hell/Desktop/term1/Project1 PR/aclImdb_v1/aclImdb/train/neg-/*.txt')\n",
    "print(len(DataPathList_Train_Neg) )  #number of items\n",
    "\n",
    "DataPathList_Test_Pos = glob.glob('C:/Users/hell/Desktop/term1/Project1 PR/test+/*.txt')\n",
    "print(len(DataPathList_Test_Pos) )  #number of items\n",
    "\n",
    "DataPathList_Test_Neg = glob.glob('C:/Users/hell/Desktop/term1/Project1 PR/test-/*.txt')\n",
    "print(len(DataPathList_Test_Neg) )  #number of items\n",
    "\n",
    "Data_Train= []\n",
    "Data_Test= []\n",
    "\n",
    "for fp in DataPathList_Train_Pos:\n",
    "\n",
    "    with open(fp, 'r') as f:\n",
    "       text = f.read()\n",
    "       text=cleanhtml(text)\n",
    "       text=nltk.word_tokenize(text)\n",
    "       text = ([[word for word in text if word not in cachedStopWords]])\n",
    "      # print(text[0])\n",
    "       model = Word2Vec(text, min_count=1, size=100)\n",
    "       #print(len(model.wv.vocab))\n",
    "       for word in text:\n",
    "           w = model[word]\n",
    "       #print(w)\n",
    "       #print([sum(x)/len(model.wv.vocab) for x in zip(*w)])\n",
    "       text = ([sum(x)/len(text) for x in zip(*w)])\n",
    "       Data_Train.append(text)\n",
    "#print(Data_Train)\n",
    "\n",
    "for fp in DataPathList_Train_Neg:\n",
    "\n",
    "    with open(fp, 'r') as f:\n",
    "       text = f.read()\n",
    "       text=cleanhtml(text)\n",
    "       text=nltk.word_tokenize(text)\n",
    "       text = ([[word for word in text if word not in cachedStopWords]])\n",
    "       #print(text[0])\n",
    "       model = Word2Vec(text, min_count=1, size=100)\n",
    "       #print(len(model.wv.vocab))\n",
    "       for word in text:\n",
    "           w = model[word]\n",
    "       #print(w)\n",
    "       #print([sum(x)/len(model.wv.vocab) for x in zip(*w)])\n",
    "       text = ([sum(x)/len(text) for x in zip(*w)])\n",
    "       Data_Train.append(text)\n",
    "\n",
    "for fp in DataPathList_Test_Pos:\n",
    "\n",
    "    with open(fp, 'r') as f:\n",
    "       text = f.read()\n",
    "       text=cleanhtml(text)\n",
    "       text=nltk.word_tokenize(text)\n",
    "       text = ([[word for word in text if word not in cachedStopWords]])\n",
    "       #print(text[0])\n",
    "       model = Word2Vec(text, min_count=1, size=100)\n",
    "       #print(len(model.wv.vocab))\n",
    "       for word in text:\n",
    "           w = model[word]\n",
    "       #print(w)\n",
    "       #print([sum(x)/len(model.wv.vocab) for x in zip(*w)])\n",
    "       text = ([sum(x)/len(text) for x in zip(*w)])\n",
    "       Data_Test.append(text)\n",
    "\n",
    "for fp in DataPathList_Test_Neg:\n",
    "\n",
    "    with open(fp, 'r') as f:\n",
    "       text = f.read()\n",
    "       text=cleanhtml(text)\n",
    "       text=nltk.word_tokenize(text)\n",
    "       text = ([[word for word in text if word not in cachedStopWords]])\n",
    "     #  print(text[0])\n",
    "       model = Word2Vec(text, min_count=1, size=100)\n",
    "     #  print(len(model.wv.vocab))\n",
    "       for word in text:\n",
    "           w = model[word]\n",
    "    #   print(w)\n",
    "       #print([sum(x)/len(model.wv.vocab) for x in zip(*w)])\n",
    "       text = ([sum(x)/len(text) for x in zip(*w)])\n",
    "       Data_Test.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Data_TrainP3.npy', Data_Train)\n",
    "np.save('Data_TestP3.npy', Data_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Train=np.load('Data_TrainP3.npy')\n",
    "Data_Test=np.load('Data_TestP3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hell\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 10 Fold : 61.47\n",
      "Accuracy with Train Test Split : 61.727272727272734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "Class = [1 if i < 5000 else 0 for i in range(10000)]\n",
    "\n",
    "data_train, data_validation, class_train, class_validation = train_test_split(\n",
    "    Data_Train, Class, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf2 = RandomForestClassifier(n_estimators=50,max_depth=5)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracy = cross_val_score(clf2, Data_Train, Class, cv=10 )\n",
    "print ('Accuracy with 10 Fold :',accuracy.mean()*100)\n",
    "\n",
    "clf2.fit(data_train, class_train )\n",
    "predicted = clf2.predict(data_validation)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve, auc\n",
    "print ('Accuracy with Train Test Split :',accuracy_score(class_validation,predicted)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set Accuracy: 63.66666666666667\n",
      "confusion_matrix:\n",
      " [[107  43]\n",
      " [ 66  84]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "Class = [1 if i < 150 else 0 for i in range(300)]\n",
    "predicted = clf2.predict(Data_Test)\n",
    "print (\"Test set Accuracy:\"  , accuracy_score(Class, predicted)*100)\n",
    "print ('confusion_matrix:\\n',confusion_matrix(Class,predicted))\n",
    "\n",
    "fpr = []\n",
    "tpr = []\n",
    "roc_auc = []\n",
    "fpr, tpr,_ = roc_curve(Class, predicted)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
